{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5fd79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True http://sec.gov/Archives/edgar/data/878828/000149315223020861/form8-k.htm\n",
      "True http://sec.gov/Archives/edgar/data/1826667/000110465923070148/tm2318362d1_8k.htm\n",
      "True http://sec.gov/Archives/edgar/data/1435049/000119312523164891/d511815d8k.htm\n",
      "True http://sec.gov/Archives/edgar/data/1120193/000119312523164839/d476077d8k.htm\n",
      "True http://sec.gov/Archives/edgar/data/1854583/000121390023047889/ea180116-8k425_abrispac1.htm\n",
      "True http://sec.gov/Archives/edgar/data/1281845/000149315223020769/form8-k.htm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163.22192072868347"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pdfkit\n",
    "import sqlite3\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "def get_all_filings(soup):\n",
    "    #Go through all filings with html text and accerssion\n",
    "    filings = []\n",
    "    for row in soup.findAll(\"tr\"):\n",
    "        if \"Accession Number\" in row.text:\n",
    "            filings.append(row)\n",
    "            \n",
    "    return filings\n",
    "\n",
    "\n",
    "def get_acc_no(text):\n",
    "    # Extract the accession number using regex\n",
    "    match = re.search(r\"Accession Number: (\\d{10}-\\d{2}-\\d{6})\", text)\n",
    "    if match:\n",
    "        accession_number = match.group(1)\n",
    "        return (accession_number)\n",
    "    \n",
    "    \n",
    "def get_filing_metadata(filing):\n",
    "    for links in filing.findAll('a'):\n",
    "        href = links['href']\n",
    "\n",
    "        if \".htm\" in href:\n",
    "            #\"click\" on link (just request that link)\n",
    "            x = requests.get(r\"http://sec.gov\" + href, headers=headers)\n",
    "\n",
    "            \n",
    "    soup = BeautifulSoup(x.text, \"html.parser\")\n",
    "    cik = re.search(r\"CIK:\\s+(\\d+)\", soup.text).group(1)\n",
    "    \n",
    "    for _ in soup.findAll('a'):\n",
    "        if \"ix?doc=\" in _['href']:\n",
    "            partial_link = _['href'].split(\"/ix?doc=\")[-1]\n",
    "\n",
    "            filing_link = \"http://sec.gov\" + partial_link\n",
    "            \n",
    "            return filing_link, cik\n",
    "        \n",
    "        \n",
    "def get_filing_time(filing):\n",
    "    time_data = filing.findAll('td')[3]\n",
    "    date = time_data.contents[0]\n",
    "    time = time_data.contents[2]\n",
    "    \n",
    "    datetime_obj = datetime.strptime(date + \" \" + time, '%Y-%m-%d %H:%M:%S')\n",
    "    unix_time = int(datetime_obj.timestamp())\n",
    "    \n",
    "    return unix_time\n",
    "        \n",
    "    \n",
    "def get_filing(filing_link):\n",
    "    raw_filing = BeautifulSoup(requests.get(filing_link, headers=headers).text, \"html.parser\").find(\"body\").text\n",
    "    filing = clean_filing(raw_filing)\n",
    "    \n",
    "    return filing\n",
    "        \n",
    "        \n",
    "def clean_filing(raw_filing):\n",
    "    filing = raw_filing.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").strip()\n",
    "    filing = \" \".join(filing.split())\n",
    "\n",
    "\n",
    "    filing = \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\" + \\\n",
    "            filing.split(\"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\")[-1]\n",
    "    \n",
    "    return filing.lower()\n",
    "\n",
    "\n",
    "def is_filing_merger(filing_text):\n",
    "    #Need to make more solid determination\n",
    "    if \"merger\" not in filing_text:\n",
    "        return False\n",
    "    \n",
    "    if \"item 1.01\".lower() in filing_text:\n",
    "        return True\n",
    "    \n",
    "    if \"item 7.01\".lower() in filing_text:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "    \n",
    "headers = {\n",
    "\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base_url = r\"https://www.sec.gov/cgi-bin/browse-edgar?company=&CIK=&type=8-K&owner=include&count=100&action=getcurrent\"\n",
    "\n",
    "\n",
    "current_dir = os.getcwd() + \"\\\\\" \n",
    "\n",
    "\n",
    "path_wkhtmltopdf = current_dir.split(\"scraper\")[0] + \"wkhtmltopdf\\\\bin\\\\wkhtmltopdf.exe\"\n",
    "config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "\n",
    "DB_PATH = current_dir.split(\"scraper\")[0] + \"database\\\\filing_data.sqlite3\"\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "latest_8k_filings = requests.get(base_url, headers=headers).text\n",
    "soup = BeautifulSoup(latest_8k_filings, \"html.parser\")\n",
    "\n",
    "#Iterate through all filings on page\n",
    "filings = get_all_filings(soup)\n",
    "\n",
    "x = time.time()\n",
    "\n",
    "\n",
    "cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "seen = cursor.fetchall()\n",
    "all_accession_numbers = {row[0] for row in seen}\n",
    "max_unix_number = max({row[1] for row in seen})\n",
    "\n",
    "for filing in filings:\n",
    "    filing_acc_no = get_acc_no(filing.findAll('td')[2].text)\n",
    "    \n",
    "    if filing_acc_no in all_accession_numbers:\n",
    "        continue\n",
    "    \n",
    "    filing_link, company_cik = get_filing_metadata(filing)\n",
    "    filing_time = get_filing_time(filing)\n",
    "    \n",
    "    \n",
    "    if max_unix_number > filing_time:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    filing_text = get_filing(filing_link)\n",
    "    \n",
    "    if is_filing_merger(filing_text.lower()):\n",
    "        print(True, filing_link)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            #Store metadata to db\n",
    "            cursor.execute(\"INSERT INTO data (accession_no,cik, unix_number) VALUES (?, ?, ?)\",\n",
    "                           (filing_acc_no, company_cik, filing_time))\n",
    "\n",
    "            # Commit the changes to the database\n",
    "            conn.commit()\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #Save as pdf to 8k folder\n",
    "        filings_path = current_dir.split(\"scraper\")[0] + \"8ks\"\n",
    "        filing_path = filings_path + filing_acc_no + \".pdf\"\n",
    "        pdfkit.from_url(filing_link, filing_path, configuration=config)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO seen_filings (accession_no, unix_number) VALUES (?, ?)\",\n",
    "                   (filing_acc_no, filing_time))\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "\n",
    "cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "seen = cursor.fetchall()\n",
    "all_accession_numbers = {row[0] for row in seen}\n",
    "max_unix_number = max({row[1] for row in seen})\n",
    "    \n",
    "conn.close()\n",
    "    \n",
    "time.time() - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6851e874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"item_0\": {\"accession_no\": \"text1\", \"cik\": \"text2\", \"unix_number\": 123}, \"item_1\": {\"accession_no\": \"12412\", \"cik\": \"214\", \"unix_number\": 643}}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(r\"http://127.0.0.1:8000/data/\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cfc26d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sbuca\\\\Desktop\\\\code_post_grad\\\\merger_arb\\\\8ks'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd().split(\"scraper\")[0] + \"8ks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "789c4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1686581368"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_unix_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "710a3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "seen = cursor.fetchall()\n",
    "all_accession_numbers = {row[0] for row in seen}\n",
    "max_unix_number = max({row[1] for row in seen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ef33f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12412', 643)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "002f81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_unix_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d255b5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_269396\\8338833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mlatest_8k_filings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatest_8k_filings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;31m#Iterate through all filings on page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# < + letter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mparse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mendpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrfind_tolerant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[0mattrname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pdfkit\n",
    "import sqlite3\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "def get_all_filings(soup):\n",
    "    #Go through all filings with html text and accerssion\n",
    "    filings = []\n",
    "    for row in soup.findAll(\"tr\"):\n",
    "        if \"Accession Number\" in row.text:\n",
    "            filings.append(row)\n",
    "            \n",
    "    return filings\n",
    "\n",
    "\n",
    "def get_acc_no(text):\n",
    "    # Extract the accession number using regex\n",
    "    match = re.search(r\"Accession Number: (\\d{10}-\\d{2}-\\d{6})\", text)\n",
    "    if match:\n",
    "        accession_number = match.group(1)\n",
    "        return (accession_number)\n",
    "    \n",
    "    \n",
    "def get_filing_metadata(filing):\n",
    "    for links in filing.findAll('a'):\n",
    "        href = links['href']\n",
    "\n",
    "        if \".htm\" in href:\n",
    "            #\"click\" on link (just request that link)\n",
    "            x = requests.get(r\"http://sec.gov\" + href, headers=headers)\n",
    "\n",
    "            \n",
    "    soup = BeautifulSoup(x.text, \"html.parser\")\n",
    "    cik = re.search(r\"CIK:\\s+(\\d+)\", soup.text).group(1)\n",
    "    \n",
    "    for _ in soup.findAll('a'):\n",
    "        if \"ix?doc=\" in _['href']:\n",
    "            partial_link = _['href'].split(\"/ix?doc=\")[-1]\n",
    "\n",
    "            filing_link = \"http://sec.gov\" + partial_link\n",
    "            \n",
    "            return filing_link, cik\n",
    "        \n",
    "        \n",
    "def get_filing_time(filing):\n",
    "    time_data = filing.findAll('td')[3]\n",
    "    date = time_data.contents[0]\n",
    "    time = time_data.contents[2]\n",
    "    \n",
    "    datetime_obj = datetime.strptime(date + \" \" + time, '%Y-%m-%d %H:%M:%S')\n",
    "    unix_time = int(datetime_obj.timestamp())\n",
    "    \n",
    "    return unix_time\n",
    "        \n",
    "    \n",
    "def get_filing(filing_link):\n",
    "    raw_filing = BeautifulSoup(requests.get(filing_link, headers=headers).text, \"html.parser\").find(\"body\").text\n",
    "    filing = clean_filing(raw_filing)\n",
    "    \n",
    "    return filing\n",
    "        \n",
    "        \n",
    "def clean_filing(raw_filing):\n",
    "    filing = raw_filing.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").strip()\n",
    "    filing = \" \".join(filing.split())\n",
    "\n",
    "\n",
    "    filing = \"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\" + \\\n",
    "            filing.split(\"UNITED STATES SECURITIES AND EXCHANGE COMMISSION\")[-1]\n",
    "    \n",
    "    return filing.lower()\n",
    "\n",
    "\n",
    "def is_filing_merger(filing_text):\n",
    "    #Need to make more solid determination\n",
    "    if \"merger\" not in filing_text:\n",
    "        return False\n",
    "    \n",
    "    if \"item 1.01\".lower() in filing_text:\n",
    "        return True\n",
    "    \n",
    "    if \"item 7.01\".lower() in filing_text:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "    \n",
    "headers = {\n",
    "\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"\n",
    "}\n",
    "\n",
    "base_url = r\"https://www.sec.gov/cgi-bin/browse-edgar?company=&CIK=&type=8-K&owner=include&count=100&action=getcurrent\"\n",
    "\n",
    "\n",
    "current_dir = os.getcwd() + \"\\\\\" \n",
    "\n",
    "\n",
    "path_wkhtmltopdf = current_dir.split(\"scraper\")[0] + \"wkhtmltopdf\\\\bin\\\\wkhtmltopdf.exe\"\n",
    "config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "\n",
    "DB_PATH = current_dir.split(\"scraper\")[0] + \"database\\\\filing_data.sqlite3\"\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "while True:\n",
    "    latest_8k_filings = requests.get(base_url, headers=headers).text\n",
    "    soup = BeautifulSoup(latest_8k_filings, \"html.parser\")\n",
    "\n",
    "    #Iterate through all filings on page\n",
    "    filings = get_all_filings(soup)\n",
    "\n",
    "    x = time.time()\n",
    "\n",
    "\n",
    "    cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "    seen = cursor.fetchall()\n",
    "    all_accession_numbers = {row[0] for row in seen}\n",
    "    max_unix_number = max({row[1] for row in seen})\n",
    "\n",
    "    for filing in filings:\n",
    "        filing_acc_no = get_acc_no(filing.findAll('td')[2].text)\n",
    "\n",
    "        if filing_acc_no in all_accession_numbers:\n",
    "            continue\n",
    "\n",
    "        filing_link, company_cik = get_filing_metadata(filing)\n",
    "        filing_time = get_filing_time(filing)\n",
    "\n",
    "\n",
    "        if max_unix_number > filing_time:\n",
    "            continue\n",
    "\n",
    "\n",
    "        filing_text = get_filing(filing_link)\n",
    "\n",
    "        if is_filing_merger(filing_text.lower()):\n",
    "            print(True, filing_link)\n",
    "\n",
    "\n",
    "            try:\n",
    "                #Store metadata to db\n",
    "                cursor.execute(\"INSERT INTO data (accession_no,cik, unix_number) VALUES (?, ?, ?)\",\n",
    "                               (filing_acc_no, company_cik, filing_time))\n",
    "\n",
    "                # Commit the changes to the database\n",
    "                conn.commit()\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #Save as pdf to 8k folder\n",
    "            filings_path = current_dir.split(\"scraper\")[0] + \"8ks\\\\\"\n",
    "            filing_path = filings_path + filing_acc_no + \".pdf\"\n",
    "            pdfkit.from_url(filing_link, filing_path, configuration=config)\n",
    "\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\"INSERT INTO seen_filings (accession_no, unix_number) VALUES (?, ?)\",\n",
    "                       (filing_acc_no, filing_time))\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "    seen = cursor.fetchall()\n",
    "    all_accession_numbers = {row[0] for row in seen}\n",
    "    max_unix_number = max({row[1] for row in seen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0688850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1686584304"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_unix_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99eb2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sbuca\\\\Desktop\\\\code_post_grad\\\\merger_arb\\\\8ks'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filings_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d34cc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sbuca\\\\Desktop\\\\code_post_grad\\\\merger_arb\\\\8ks0001493152-23-020769.pdf'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "348be93b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_269396\\2035424515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mall_accession_numbers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmax_unix_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "    cursor.execute(\"SELECT accession_no, unix_number FROM seen_filings\")\n",
    "    seen = cursor.fetchall()\n",
    "    all_accession_numbers = {row[0] for row in seen}\n",
    "    max_unix_number = max({row[1] for row in seen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbbbe502",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d87e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
